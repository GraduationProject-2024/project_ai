#from sentence_transformers import SentenceTransformer
#import torch
#import torch.nn as nn
#import torch.optim as optim
#import numpy as np
#from sklearn.preprocessing import StandardScaler
#from sklearn.metrics.pairwise import cosine_similarity

#class HospitalRecommender:
#    def __init__(self, model_name='sentence-transformers/distiluse-base-multilingual-cased-v2'):
#        self.embedding_model = SentenceTransformer(model_name)
#        #https://brunch.co.kr/@b2439ea8fc654b8/70 ì°¸ê³ í•´ì„œ ëª¨ë¸ ê²°ì •(ê²½ëŸ‰í™”ëœ ëª¨ë¸)

    
#    def train_vae(self, data, input_dim, hidden_dim, latent_dim, epochs=100, lr=0.001):
#        """
#        Variational Autoencoder (VAE) í•™ìŠµ
#        :param data: ì…ë ¥ ë°ì´í„° (numpy array)
#        :param input_dim: ì…ë ¥ ë°ì´í„° ì°¨ì›
#        :param hidden_dim: ì¤‘ê°„ì¸µ ì°¨ì›
#        :param latent_dim: ì ì¬ ê³µê°„ ì°¨ì›
#        :param epochs: í•™ìŠµ ë°˜ë³µ íšŸìˆ˜
#        :param lr: í•™ìŠµë¥ 
#        :return: í•™ìŠµëœ VAE ëª¨ë¸
#        """
#        #VAE ëª¨ë¸ ì •ì˜
#        class VAE(nn.Module):
#            def __init__(self, input_dim, hidden_dim, latent_dim):
#                super(VAE, self).__init__()
#                self.encoder = nn.Sequential(
#                    nn.Linear(input_dim, hidden_dim),
#                    nn.LeakyReLU()
#                )
#                self.mu_layer = nn.Linear(hidden_dim, latent_dim)
#                self.log_var_layer = nn.Linear(hidden_dim, latent_dim)
#                self.decoder = nn.Sequential(
#                    nn.Linear(latent_dim, hidden_dim),
#                    nn.LeakyReLU(),
#                    nn.Linear(hidden_dim, input_dim),
#                    nn.Sigmoid()
#                )

#            def encode(self, x):
#                h = self.encoder(x)
#                mu = self.mu_layer(h)
#                log_var = self.log_var_layer(h)
#                return mu, log_var

#            def reparameterize(self, mu, log_var):
#                std = torch.exp(0.5 * log_var)
#                eps = torch.randn_like(std)
#                return mu + eps * std

#            def decode(self, z):
#                return self.decoder(z)

#            def forward(self, x):
#                mu, log_var = self.encode(x)
#                z = self.reparameterize(mu, log_var)
#                reconstructed = self.decode(z)
#                return reconstructed, mu, log_var

#        #VAE ëª¨ë¸ ì´ˆê¸°í™”
#        vae_model = VAE(input_dim, hidden_dim, latent_dim)
#        optimizer = optim.Adam(vae_model.parameters(), lr=lr)
#        criterion = nn.MSELoss(reduction='sum')  #Reconstruction Loss

#        #ë°ì´í„°ë¥¼ PyTorch Tensorë¡œ ë³€í™˜
#        tensor_data = torch.tensor(data, dtype=torch.float32)

#        #í•™ìŠµ ë£¨í”„
#        for epoch in range(epochs):
#            vae_model.train()
#            optimizer.zero_grad()

#            reconstructed, mu, log_var = vae_model(tensor_data)
#            #ì†ì‹¤ ê³„ì‚°: Reconstruction Loss + KL Divergence
#            recon_loss = criterion(reconstructed, tensor_data)
#            kl_div = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())
#            loss = recon_loss + kl_div

#            loss.backward()
#            optimizer.step()

#            #print(f"Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}, Recon Loss: {recon_loss.item()}, KL Div: {kl_div.item()}")

#        return vae_model

#    #ì‚¬ìš©ì í”„ë¡œí•„ ì„ë² ë”© í•¨ìˆ˜
#    def embed_user_profile(self, basic_info, health_info):
#        text_data = " ".join([
#            health_info["pastHistory"] or "",
#            health_info["familyHistory"] or "",
#            health_info["nowMedicine"] or "",
#            health_info["allergy"] or ""
#        ])
#        text_embedding = self.embedding_model.encode([text_data])

#        numeric_features = np.array([basic_info["age"], basic_info["height"], basic_info["weight"]])
#        norm = np.linalg.norm(numeric_features)
#        numeric_embedding = numeric_features / norm if norm != 0 else numeric_features

        
#        return np.hstack((text_embedding[0], numeric_embedding))

#    #ë³‘ì› ë°ì´í„° ì„ë² ë”© í•¨ìˆ˜
#    def embed_hospital_data(self, hospitals_df, suspected_disease=None):
#        #ì§„ë£Œê³¼ í…ìŠ¤íŠ¸ ë°ì´í„° ë²¡í„°í™”
#        department_embeddings = self.embedding_model.encode(hospitals_df["department"].fillna("").tolist())

#        #ë³‘ì› ìœ í˜• í…ìŠ¤íŠ¸ ì„ë² ë”©
#        clcdnm_embeddings = self.embedding_model.encode(hospitals_df["clcdnm"].fillna("").tolist())

#        #ë³‘ì› ì´ë¦„ í…ìŠ¤íŠ¸ ì„ë² ë”© ì¶”ê°€
#        name_embeddings = self.embedding_model.encode(hospitals_df["name"].fillna("").tolist())

#        #ë³‘ì› ì†Œìš” ì‹œê°„ ë°ì´í„° ì •ê·œí™”
#        scaler = StandardScaler()
#        time_distance_features = hospitals_df[[
#            "transit_travel_time_h",
#            "transit_travel_time_m",
#            "transit_travel_time_s",
#            "transit_travel_distance_km"
#        ]].fillna(0).values
#        time_distance_embeddings = scaler.fit_transform(time_distance_features)

#        #ì˜ì‹¬ ì§ˆë³‘ ì„ë² ë”©(ì„ íƒ)
#        #if suspected_disease:
#        #   suspected_disease_embedding = self.embedding_model.encode([suspected_disease])[0]
#        #   suspected_disease_embeddings = np.tile(suspected_disease_embedding, (hospitals_df.shape[0], 1))
#        #else:
#        #   suspected_disease_embeddings = np.zeros((hospitals_df.shape[0], department_embeddings.shape[1]))
#        #suspected_diseaseë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ê°•ì œ ë³€í™˜
#        if suspected_disease:
#            if isinstance(suspected_disease, str):
#                suspected_disease = [suspected_disease]  #ë‹¨ì¼ ê°’ì¼ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜

#            #ì—¬ëŸ¬ ì˜ì‹¬ ì§ˆë³‘ì„ ë²¡í„°í™”í•˜ê³  í‰ê· ê°’ ì‚¬ìš©
#            suspected_disease_embeddings = self.embedding_model.encode(suspected_disease)
#            avg_suspected_embedding = np.mean(suspected_disease_embeddings, axis=0)

#            suspected_disease_embeddings = np.tile(avg_suspected_embedding, (hospitals_df.shape[0], 1))
#        else:
#            suspected_disease_embeddings = np.zeros((hospitals_df.shape[0], department_embeddings.shape[1]))


#        #ìµœì¢… ë³‘ì› ë²¡í„° ê²°í•©
#        return np.hstack((department_embeddings, 
#                          clcdnm_embeddings, 
#                          name_embeddings, 
#                          suspected_disease_embeddings, 
#                          time_distance_embeddings))

import openai
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.metrics.pairwise import cosine_similarity
import torch
import torch.nn as nn
import torch.optim as optim

import configparser
config = configparser.ConfigParser()
config.read('keys.config')
openai.api_key = config['API_KEYS']['chatgpt_api_key']

class HospitalRecommender:
    def __init__(self, model_name='text-embedding-3-small'):
        self.model_name = model_name  # OpenAI Embedding Model ì‚¬ìš©

    def train_vae(self, data, input_dim, hidden_dim, latent_dim, epochs=100, lr=0.001):
        """
        Variational Autoencoder (VAE) í•™ìŠµ
        :param data: ì…ë ¥ ë°ì´í„° (numpy array)
        :param input_dim: ì…ë ¥ ë°ì´í„° ì°¨ì›
        :param hidden_dim: ì¤‘ê°„ì¸µ ì°¨ì›
        :param latent_dim: ì ì¬ ê³µê°„ ì°¨ì›
        :param epochs: í•™ìŠµ ë°˜ë³µ íšŸìˆ˜
        :param lr: í•™ìŠµë¥ 
        :return: í•™ìŠµëœ VAE ëª¨ë¸
        """

        class VAE(nn.Module):
            def __init__(self, input_dim, hidden_dim, latent_dim):
                super(VAE, self).__init__()
                self.encoder = nn.Sequential(
                    nn.Linear(input_dim, hidden_dim),
                    nn.LeakyReLU()
                )
                self.mu_layer = nn.Linear(hidden_dim, latent_dim)
                self.log_var_layer = nn.Linear(hidden_dim, latent_dim)
                self.decoder = nn.Sequential(
                    nn.Linear(latent_dim, hidden_dim),
                    nn.LeakyReLU(),
                    nn.Linear(hidden_dim, input_dim),
                    nn.Sigmoid()
                )

            def encode(self, x):
                h = self.encoder(x)
                mu = self.mu_layer(h)
                log_var = self.log_var_layer(h)
                return mu, log_var

            def reparameterize(self, mu, log_var):
                std = torch.exp(0.5 * log_var)
                eps = torch.randn_like(std)
                return mu + eps * std

            def decode(self, z):
                return self.decoder(z)

            def forward(self, x):
                mu, log_var = self.encode(x)
                z = self.reparameterize(mu, log_var)
                reconstructed = self.decode(z)
                return reconstructed, mu, log_var

        # VAE ëª¨ë¸ ì´ˆê¸°í™”
        vae_model = VAE(input_dim, hidden_dim, latent_dim)
        optimizer = optim.Adam(vae_model.parameters(), lr=lr)
        criterion = nn.MSELoss(reduction='sum')  # Reconstruction Loss

        # ë°ì´í„°ë¥¼ PyTorch Tensorë¡œ ë³€í™˜
        tensor_data = torch.tensor(data, dtype=torch.float32)


            # í•™ìŠµ ë£¨í”„
        for epoch in range(epochs):
            vae_model.train()
            optimizer.zero_grad()

            reconstructed, mu, log_var = vae_model(tensor_data)
            recon_loss = criterion(reconstructed, tensor_data)
            kl_div = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())
            loss = recon_loss + kl_div

            loss.backward()
            optimizer.step()

        return vae_model
        
    def get_embedding(self, text_list, batch_size=256):
        """
        OpenAI APIë¥¼ ì´ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì„ë² ë”©ì„ ìƒì„±
        :param text_list: ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì˜ ì…ë ¥ í…ìŠ¤íŠ¸ ë°ì´í„°
        :return: numpy array í˜•íƒœì˜ ì„ë² ë”© ê²°ê³¼
        """
        # if not isinstance(texts, list):  
        #     texts = [str(texts)]  #ë‹¨ì¼ ë¬¸ìì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        # else:
        #     texts = [str(text).strip() for text in texts if text and isinstance(text, str)]  #ë¹ˆ ë¬¸ìì—´ ì œê±°

        # if not texts:  #ë¹„ì–´ ìˆëŠ” ê²½ìš° ê¸°ë³¸ê°’ ì œê³µ
        #     texts = ["unknown"]

        # # **ë””ë²„ê¹…: ì…ë ¥ ë°ì´í„° í™•ì¸**
        # print(f"ğŸ”¹ OpenAI API í˜¸ì¶œ - ì…ë ¥ ë°ì´í„° í™•ì¸: {texts[:5]}")  # ì²˜ìŒ 5ê°œë§Œ ì¶œë ¥
        # print(f"ğŸ”¹ ì…ë ¥ ë°ì´í„° ê¸¸ì´: {len(texts)}")
        
        # try:
        #     response = openai.embeddings.create(
        #         model=self.model_name,
        #         input=texts,
        #         encoding_format="float"  # float í˜•ì‹ìœ¼ë¡œ ë²¡í„° ë°˜í™˜
        #     )
        #     # **ë””ë²„ê¹…: ì‘ë‹µ ë°ì´í„° í™•ì¸**
        #     print(f"âœ… OpenAI API ì‘ë‹µ ê¸¸ì´: {len(response.data)}")
        #     return np.array([embedding.embedding for embedding in response.data])
        # except openai.BadRequestError as e:
        #     print(f"âŒ OpenAI API ìš”ì²­ ì‹¤íŒ¨: {e}")
        #     return np.zeros((len(texts), 1536))  # ê¸°ë³¸ Embedding í¬ê¸°ë¡œ ë¹ˆ ë²¡í„° ë°˜í™˜ (1536ì€ OpenAI ada-002ì˜ ì„ë² ë”© ì°¨ì›)
        embeddings = []
        for i in range(0, len(text_list), batch_size):
            batch = text_list[i : i + batch_size]
            try:
                response = openai.embeddings.create(
                    model=self.model_name,  # ìµœì‹  ëª¨ë¸ ì‚¬ìš© ì¶”ì²œ
                    input=batch,
                    encoding_format="float"
                )
                #print("âœ… OpenAI API ì‘ë‹µ ì„±ê³µ:", response)
                embeddings.extend([embedding.embedding for embedding in response.data])

            except openai.OpenAIError as e:
                #print(f"âŒ OpenAI API ìš”ì²­ ì‹¤íŒ¨: {e}")
                # ìš”ì²­ ì‹¤íŒ¨ ì‹œ, ë¹ˆ ë²¡í„° ë°˜í™˜ (1536ì€ text-embedding-ada-002 ëª¨ë¸ì˜ ê¸°ë³¸ ì°¨ì›)
                zero_vector = np.zeros((len(batch), 1536))
                embeddings.extend(zero_vector)

        return np.array(embeddings)

    def embed_user_profile(self, basic_info, health_info):
        """
        ì‚¬ìš©ì ê±´ê°• ì •ë³´ ë° ê¸°ì´ˆ ì •ë³´ë¥¼ OpenAI Embeddingì„ í™œìš©í•˜ì—¬ ë²¡í„°í™”
        """
        text_data = [
            health_info.get("pastHistory", "unknown"),
            health_info.get("familyHistory", "unknown"),
            health_info.get("nowMedicine", "unknown"),
            health_info.get("allergy", "unknown")
        ]
        #OpenAI APIëŠ” 2048ì ì œí•œ â†’ ê° í•­ëª©ì„ ê°œë³„ì ìœ¼ë¡œ ê²€ì‚¬í•˜ì—¬ ì˜ë¼ì¤Œ
        #í‰ê·  ì„ë² ë”© ê³„ì‚° (ë¦¬ìŠ¤íŠ¸ì˜ ëª¨ë“  ë²¡í„° í‰ê· )
        text_data = [text[:512] for text in text_data]  #ë¬¸ì¥ì´ ë„ˆë¬´ ê¸¸ë©´ 512ìë¡œ ì œí•œ

        text_embedding = self.get_embedding([text_data])  #ë‹¨ì¼ ë¬¸ì¥
        text_embedding = np.mean(text_embedding, axis=0)

        #ë‚˜ì´, í‚¤, ëª¸ë¬´ê²Œ ì •ê·œí™” í›„ ê²°í•©
        # numeric_features = np.array([basic_info["age"], basic_info["height"], basic_info["weight"]])
        numeric_features = np.array([
            basic_info.get("age", 0), 
            basic_info.get("height", 0), 
            basic_info.get("weight", 0)
        ])
        norm = np.linalg.norm(numeric_features)
        numeric_embedding = numeric_features / norm if norm != 0 else numeric_features

        return np.hstack((text_embedding, numeric_embedding))

    def embed_hospital_data(self, hospitals_df, suspected_disease=None):
        """
        ë³‘ì› ë°ì´í„°ë¥¼ OpenAI API ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
        """
        for col in hospitals_df.columns:
            if hospitals_df[col].dtype == "object":
                #print("object ì¹¼ëŸ¼:", hospitals_df[col])
                hospitals_df[col] = hospitals_df[col].fillna("unknown").replace("", "unknown")
            elif hospitals_df[col].dtype in ["float64", "int64"]:
                #print("numberic ì¹¼ëŸ¼:", hospitals_df[col])
                hospitals_df[col] = hospitals_df[col].fillna(0)
                
        # ëª¨ë“  ê¸°ë³¸ê°’ì„ "unknown"ìœ¼ë¡œ ì„¤ì •
        # hospital_texts = hospitals_df["name"].tolist()
        # department_texts = hospitals_df["department"].tolist()
        # clcdnm_texts = hospitals_df["clcdnm"].tolist()

        # # ğŸ”¹ OpenAI APIì— ì „ë‹¬í•  í…ìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
        # hospital_texts = [" | ".join(text.split(",")) for text in hospitals_df["name"].tolist()]  # âœ… ì‰¼í‘œ ì œê±°
        # department_texts = [" | ".join(text.split(",")) for text in hospitals_df["department"].tolist()]  # âœ… ì‰¼í‘œ ì œê±°
        # clcdnm_texts = hospitals_df["clcdnm"].tolist()

        # #OpenAI API í˜¸ì¶œì„ í†µí•´ ë²¡í„°í™” (Batch ìš”ì²­)
        # hospital_embeddings = self.get_embedding(hospital_texts)
        # department_embeddings = self.get_embedding(department_texts)
        # clcdnm_embeddings = self.get_embedding(clcdnm_texts)
        
        #ë³‘ì› ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ ë¬¸ì¥ìœ¼ë¡œ ê²°í•©í•˜ì—¬ OpenAI APIë¡œ ë²¡í„°í™”

        hospital_sentences = hospitals_df.apply(
            lambda row: f"{row['name']}ì€(ëŠ”) {row['sidocdnm']} {row['sggucdnm']}ì— ìœ„ì¹˜í•œ {row['clcdnm']}ì…ë‹ˆë‹¤. "
                        f"ì§„ë£Œ ê³¼ëª©ìœ¼ë¡œëŠ” {row['department']}ì´(ê°€) ìˆìœ¼ë©°, ì£¼ì†ŒëŠ” {row['address']}ì…ë‹ˆë‹¤.",
            axis=1
        ).tolist()

        # OpenAI API í˜¸ì¶œ (ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬)
        hospital_embeddings = self.get_embedding(hospital_sentences, batch_size=256)

        #ì‹œê°„ ë° ê±°ë¦¬ ì •ë³´ ì •ê·œí™”
        scaler = StandardScaler()
        time_distance_features = hospitals_df[[
            "transit_travel_time_h",
            "transit_travel_time_m",
            "transit_travel_time_s",
            "transit_travel_distance_km"
        ]].fillna(0).values
        time_distance_embeddings = scaler.fit_transform(time_distance_features)

        #ì˜ì‹¬ ì§ˆë³‘ ì²˜ë¦¬
        if suspected_disease:
            if isinstance(suspected_disease, str):
                suspected_disease = [suspected_disease]

            suspected_disease_embeddings = self.get_embedding(suspected_disease, batch_size=16)
            avg_suspected_embedding = np.mean(suspected_disease_embeddings, axis=0)
            suspected_disease_embeddings = np.tile(avg_suspected_embedding, (hospitals_df.shape[0], 1))
        else:
            suspected_disease_embeddings = np.zeros((hospitals_df.shape[0], hospital_embeddings.shape[1]))

        #ìµœì¢… ë³‘ì› ë²¡í„° ê²°í•©
        return np.hstack((hospital_embeddings, suspected_disease_embeddings, time_distance_embeddings))

    #ì¶”ì²œ í•¨ìˆ˜
    def recommend_hospitals(self, user_embedding, hospital_embeddings, hospitals_df, vae=None, department=None, suspected_disease=None, use_vae=False):
        #ì‚¬ìš©ì ì„ë² ë”© ì°¨ì› ë§ì¶”ê¸° (Zero Padding)
        if user_embedding.shape[0] < hospital_embeddings.shape[1]:
            padding = hospital_embeddings.shape[1] - user_embedding.shape[0]
            user_embedding_padded = np.pad(user_embedding, (0, padding))
        else:
            user_embedding_padded = user_embedding

        if use_vae:
            #vae ì‚¬ìš©í•´ì„œ latent spaceë¡œ ë³€í™˜
            user_tensor = torch.tensor(user_embedding_padded, dtype=torch.float32).unsqueeze(0)
            hospital_tensor = torch.tensor(hospital_embeddings, dtype=torch.float32)

            user_latent, _ = vae.encode(user_tensor)#autoencoder(user_tensor)
            hospital_latents, _ = vae.encode(hospital_tensor)#autoencoder(hospital_tensor)

            #ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarities = cosine_similarity(user_latent.detach().numpy(), hospital_latents.detach().numpy())
        else:
            #vaeë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì§ì ‘ ê³„ì‚°
            similarities = cosine_similarity([user_embedding_padded], hospital_embeddings)

        #ìœ ì‚¬ë„ ê²°ê³¼ë¥¼ ë³‘ì› ë°ì´í„°í”„ë ˆì„ì— ì¶”ê°€
        hospitals_df["similarity"] = similarities[0]

        #department ìœ ì‚¬ë„ ì¶”ê°€(ì„ íƒ)
        # if department:
        #     if department == "ì¹˜ì˜ê³¼":
        #         hospitals_df["department_match"] = hospitals_df["name"].apply(lambda name: "ì¹˜ê³¼" in name if name else False)
        #     elif department == "í•œë°©ê³¼":
        #         hospitals_df["department_match"] = hospitals_df["name"].apply(lambda name: any(x in name for x in ["í•œì˜ì›", "í•œë°©"]) if name else False)
        #     else:
        #         hospitals_df["department_match"] = hospitals_df["name"].apply(lambda name: department in name if name else False)
            
        # #ê°€ì¤‘ì¹˜ ë¶€ì—¬
        # hospitals_df["similarity"] += hospitals_df["department_match"] * 0.00001  #ê°€ì¤‘ì¹˜ 0.00001 ì¶”ê°€

        #suspected_diseaseë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ê°•ì œ ë³€í™˜
        if suspected_disease:
            if isinstance(suspected_disease, str):
                suspected_disease = [suspected_disease]  #ë‹¨ì¼ ê°’ì¼ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜

            #ì—¬ëŸ¬ ì˜ì‹¬ ì§ˆë³‘ì„ ë²¡í„°í™”í•˜ê³  í‰ê· ê°’ ì‚¬ìš©
            disease_embeddings = self.get_embedding(suspected_disease)
            avg_disease_embedding = np.mean(disease_embeddings, axis=0)

            hospital_name_embeddings = self.get_embedding(hospitals_df["name"].fillna("unknown").tolist())
            disease_similarities = cosine_similarity([avg_disease_embedding], hospital_name_embeddings)[0]

            hospitals_df["similarity"] += disease_similarities * 0.0001  #ê°€ì¤‘ì¹˜ ì¶”ê°€
            
        #similarity ê°’ì´ 1ì„ ì´ˆê³¼í•˜ì§€ ì•Šë„ë¡ ì œí•œ
        hospitals_df["similarity"] = hospitals_df["similarity"].clip(upper=1.0)
        #ìœ ì‚¬ë„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        #recommended = hospitals_df.sort_values(by="similarity", ascending=False)
        #recommended = recommended.reset_index(drop=True)
        return recommended